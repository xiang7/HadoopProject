\documentclass{article}
\begin{document}

\title{Project Report}
\author{Luojie Xiang, Junchao Yan}
\date{}
\maketitle

\section{Introduction}
In this project, we will extract the topics from a corpus collected from Stack Overflow using Latent Dirichlet Allocation (LDA) on a hadoop cluster. LDA is a statistical model for discovering underlying topics from a collection of documents \cite{blei2003latent}. 

\section{Experiment}
\subsection{Dataset}
The dataset of this project is obtained from Kaggle (www.kaggle.com), which is a platform for data analysis and prediction competitions. The data that we use are posted by Facebook for a keyword extraction competition. The dataset consists the files both for training and testing, of which the training file contains four columns id, title, body, and tags. In this project, only the title and body are used to extract the topics.\\[10pt]
Example:
\begin{verbatim}
id: 1

title: How to check if an uploaded file is an image without mime type?

content:

<p>I'd like to check if an uploaded file is an image file (e.g png, jpg, 
jpeg, gif, bmp) or another file. The problem is that I'm using Uploadify 
to upload the files, which changes the mime type and gives a 'text/octal' 
or something as the mime type, no matter which file type you upload.</p>

<p>Is there a way to check if the uploaded file is an image apart from
 checking the file extension using PHP?</p>

tags: php image-processing file-upload upload mime-types
\end{verbatim}

\subsection{Data Cleaning}

Describes the steps we take to do data cleaning.

remove content within \verb+<code>+

remove tags \verb+"<[^>]*>"+

Remove punctuation \verb+"[!@#$%^&*()-\_:;"'?/.,<>]"+

Remove new lines \verb+"\n" to ""+

Remove multiple white spaces \verb-"\s+" to " "-

Remove words shorter than length 3

Turn all letters to lowercase
\subsection{Hadoop Setup}

Describes how we set up our 6 node hadoop cluster.

\subsection{Latent Dirichlet Allocation}

\subsection{Results}

\section{LDA Topics}

We ran the LDA on the preprocessed text. We do a sampling during preprocessing the text because using all the entries for training a LDA model would be too much both in terms of storage space and also time. Further, according to Blei etc. \cite{blei2003latent}, LDA model performance plateaus with increasing training set size as all other machine learning models. They used 8000 data points and foound out the model is trained sufficiently long before using the entire dataset. Therefore, to avoid over-training the model, we randomly sampled 4145 data entry for LDA model training. We present the result for 10 topics as below:


\begin{center}
  \begin{tabular}{| l | l | l |}
    \hline
    Topic number & Top 10 words & Topic summary \\ \hline
    1 & time code run test start program process data memory thread & Program testing, measuring \\ \hline
    2 & page html php form http jquery javascript post url request & Web programing \\ \hline
    3 & file error code xml line python string script output files  & N/A \\ \hline
    4 & windows server system version error directory install folder build linux & System programming \\ \hline
    5 & class object method function array call type return variable instance & OOP \\ \hline
    6 & number question set amp frac mathbb find int sum point & N/A \\ \hline
    7 & java server android apache eclipse http sun attrs hibernate jar & Java \\ \hline
    8 & data database table sql id query list row column mysql & Database \\ \hline
    9 & view image button text images display click background menu layout & HTML programming \\ \hline
    10 & user app application server api client google access facebook & App \\
    \hline
  \end{tabular}
\end{center}


\section{Local Experiment}

Local experiement is a sequential execution of all essential steps. There are three major steps:

\begin{itemize}
	\item \textbf{Preprocessing.} Preprocessing the text with the steps described in the previous section. The output is a single file with individual documents taking one line in the file.
	\item \textbf{LDA input preparation.} Mahout's LDA takes a specific form of input. It comes with a utility to turn a text file into sequence of entries and then process them further into numeric vectors which is the input form of LDA model training. Therefore, the output from the prepocessing step need to be turned into a form of one entry per physical file which is the input format of the utility function of mahout.
	\item \textbf{LDA model training.} After the input of LDA model training is prepared, it is inputed into mahout and an LDA model will be trained. A utility function is provided within mahout to provide transformation from numerical vectors into original words. We wrote a simple function to extract words with the highest scores in each topic and the result is shown in next section.
\end{itemize}

\section{Hadoop Experiment}

Hadoop experiment uses MapReduce to do all the three steps in local experiment.

For the preprocessing, since each document is preprocessed in exactly the same way and the preprocessing of each document is independent. Thus, this perfectly fits into MapReduce platform. We adapted the local preprocessing code to fit the MapReduce framework. A mapper takes in a chunck of text, splits it into entries and then preprocess each entry. A sampling is performed during emittion, the reason is explained in results session. We only emit the preprocessed entry by a predefined probability. The reducer putss each entry into a separate file.

For the LDA input preparation and LDA model training, MapReduce framework is already supported in mahout. The only thing difference is we use a different set of commands with hadoop. 

\section{Conclusion}
\bibliographystyle{plain}
\bibliography{ref}
\end{document}
